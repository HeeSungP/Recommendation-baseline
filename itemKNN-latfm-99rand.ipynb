{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances, manhattan_distances\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_user : 23566, n_item : 48123\n",
      "train : 3011197, test : 23566\n"
     ]
    }
   ],
   "source": [
    "filename=\"lastfm\" # mlsmall ml1m lastfm abook\n",
    "\n",
    "#train data load\n",
    "train_data_df = pd.read_csv(\n",
    "    './data/'+filename+'.train.rating', \n",
    "    sep='\\t', header=None, names=['user', 'item'], \n",
    "    usecols=[0, 1], dtype={0: np.int32, 1: np.int32})\n",
    "\n",
    "#test data load\n",
    "#99개는 나중에 test과정에서 랜덤으로 뽑자\n",
    "test_data_df = pd.read_csv(\n",
    "    './data/'+filename+'.test.rating', \n",
    "    sep='\\t', header=None, names=['user', 'item'], \n",
    "    usecols=[0, 1], dtype={0: np.int32, 1: np.int32})\n",
    "\n",
    "#user, item num\n",
    "num_users = train_data_df['user'].max() + 1\n",
    "num_items = train_data_df['item'].max() + 1\n",
    "\n",
    "print(\"n_user : {}, n_item : {}\".format(num_users, num_items))\n",
    "print(\"train : {}, test : {}\".format(len(train_data_df), len(test_data_df)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 매트릭스 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████| 3011197/3011197 [00:06<00:00, 463446.61it/s]\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 8.45 GiB for an array with shape (48123, 23566) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-9901c4e7f0dc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;31m# item similarity matrix\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[0mitem_sim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcosine_similarity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muser_item_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0muser_item_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[0mitem_sim_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem_sim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tf_keras_nogpu\\lib\\site-packages\\sklearn\\metrics\\pairwise.py\u001b[0m in \u001b[0;36mcosine_similarity\u001b[1;34m(X, Y, dense_output)\u001b[0m\n\u001b[0;32m   1183\u001b[0m         \u001b[0mY_normalized\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX_normalized\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1184\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1185\u001b[1;33m         \u001b[0mY_normalized\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1186\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1187\u001b[0m     K = safe_sparse_dot(X_normalized, Y_normalized.T,\n",
      "\u001b[1;32m~\\.conda\\envs\\tf_keras_nogpu\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     70\u001b[0m                           FutureWarning)\n\u001b[0;32m     71\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tf_keras_nogpu\\lib\\site-packages\\sklearn\\preprocessing\\_data.py\u001b[0m in \u001b[0;36mnormalize\u001b[1;34m(X, norm, axis, copy, return_norm)\u001b[0m\n\u001b[0;32m   1708\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"'%d' is not a supported axis\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1709\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1710\u001b[1;33m     X = check_array(X, accept_sparse=sparse_format, copy=copy,\n\u001b[0m\u001b[0;32m   1711\u001b[0m                     estimator='the normalize function', dtype=FLOAT_DTYPES)\n\u001b[0;32m   1712\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0maxis\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tf_keras_nogpu\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     70\u001b[0m                           FutureWarning)\n\u001b[0;32m     71\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tf_keras_nogpu\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[0;32m    662\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    663\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcopy\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmay_share_memory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marray_orig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 664\u001b[1;33m         \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    665\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    666\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 8.45 GiB for an array with shape (48123, 23566) and data type float64"
     ]
    }
   ],
   "source": [
    "u_lst = train_data_df['user'].tolist()\n",
    "i_lst = train_data_df['item'].tolist()\n",
    "\n",
    "# rating array 생성\n",
    "mat = np.zeros((num_users, num_items))\n",
    "for i in tqdm(range(len(train_data_df))):\n",
    "    mat[u_lst[i], i_lst[i]] = 1\n",
    "\n",
    "# user - item matrix    \n",
    "user_item_df = pd.DataFrame(mat)\n",
    "\n",
    "# item similarity matrix\n",
    "item_sim = cosine_similarity(user_item_df.transpose(),user_item_df.transpose())\n",
    "item_sim_df = pd.DataFrame(item_sim)\n",
    "\n",
    "# interaction에 따른 score matrix\n",
    "score = np.zeros((num_users, num_items))\n",
    "for k in tqdm(range(len(train_data_df))):\n",
    "    u,i=u_lst[k],i_lst[k]\n",
    "    score[u]=np.array([x+y for x,y in zip(score[u],item_sim[i])])\n",
    "score_df=pd.DataFrame(score)\n",
    "\n",
    "#score_df -> dict로 변경\n",
    "#score_dict = score_df.to_dict()\n",
    "#그냥 to_dict로 변경하니까 item이 key값으로 나옴\n",
    "score_dict = score_df.transpose().to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hit(gt_item, pred_items):\n",
    "\tif gt_item in pred_items:\n",
    "\t\treturn 1\n",
    "\treturn 0\n",
    "\n",
    "\n",
    "def ndcg(gt_item, pred_items):\n",
    "\tif gt_item in pred_items:\n",
    "\t\tindex = pred_items.index(gt_item)\n",
    "\t\treturn np.reciprocal(np.log2(index+2))\n",
    "\treturn 0\n",
    "\n",
    "def evaluate(gt_item, full_pred_items, K):\n",
    "    pred_items = full_pred_items[0:K]\n",
    "    return hit(gt_item, pred_items), ndcg(gt_item, pred_items)\n",
    "\n",
    "def user_test(test_user, K):\n",
    "    # 강좌별 score\n",
    "    pred = dict(sorted(score_dict[test_user].items(),key=(lambda x:x[1]), reverse=True))\n",
    "\n",
    "    # test 100개 리스트 만들기\n",
    "    asis = train_data_df[train_data_df['user']==test_user]['item'].tolist()\n",
    "    gt = test_data_df[test_data_df['user']==test_user]['item'].tolist()\n",
    "    \n",
    "    full = set(range(0,num_items))\n",
    "    test_cand_99 = random.sample(list(full-set(asis)-set(gt)),99)\n",
    "    test_cand = gt.copy()\n",
    "    test_cand.extend(test_cand_99)\n",
    "    \n",
    "    # 100개 score 다시 뽑아서 test_score에 저장\n",
    "    test_score=dict()\n",
    "    for item in test_cand:\n",
    "        test_score.update({item:pred[item]})\n",
    "\n",
    "    res = dict(sorted(test_score.items(),key=(lambda x:x[1]), reverse=True))\n",
    "\n",
    "    return evaluate(gt[0], list(res.keys()), K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fin_hr=[]\n",
    "fin_ndcg=[]\n",
    "for epoch in tqdm(range(10)):\n",
    "    _hr=[]\n",
    "    _ndcg=[]\n",
    "\n",
    "    #for i in tqdm(range(num_users)):\n",
    "    for i in range(num_users):\n",
    "        temp1, temp2 = user_test(i,5)\n",
    "        _hr.append(temp1)\n",
    "        _ndcg.append(temp2)\n",
    "    fin_hr.append(sum(_hr)/len(_hr))\n",
    "    fin_ndcg.append(sum(_ndcg)/len(_ndcg))\n",
    "    #print(epoch+1, sum(_hr)/len(_hr), sum(_ndcg)/len(_ndcg))\n",
    "    \n",
    "print(sum(fin_hr)/len(fin_hr))\n",
    "print(sum(fin_ndcg)/len(fin_ndcg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fin_hr=[]\n",
    "fin_ndcg=[]\n",
    "for epoch in tqdm(range(10)):\n",
    "    _hr=[]\n",
    "    _ndcg=[]\n",
    "\n",
    "    #for i in tqdm(range(num_users)):\n",
    "    for i in range(num_users):\n",
    "        temp1, temp2 = user_test(i,10)\n",
    "        _hr.append(temp1)\n",
    "        _ndcg.append(temp2)\n",
    "    fin_hr.append(sum(_hr)/len(_hr))\n",
    "    fin_ndcg.append(sum(_ndcg)/len(_ndcg))\n",
    "    #print(epoch+1, sum(_hr)/len(_hr), sum(_ndcg)/len(_ndcg))\n",
    "    \n",
    "print(sum(fin_hr)/len(fin_hr))\n",
    "print(sum(fin_ndcg)/len(fin_ndcg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fin_hr=[]\n",
    "fin_ndcg=[]\n",
    "for epoch in tqdm(range(10)):\n",
    "    _hr=[]\n",
    "    _ndcg=[]\n",
    "\n",
    "    #for i in tqdm(range(num_users)):\n",
    "    for i in range(num_users):\n",
    "        temp1, temp2 = user_test(i,20)\n",
    "        _hr.append(temp1)\n",
    "        _ndcg.append(temp2)\n",
    "    fin_hr.append(sum(_hr)/len(_hr))\n",
    "    fin_ndcg.append(sum(_ndcg)/len(_ndcg))\n",
    "    #print(epoch+1, sum(_hr)/len(_hr), sum(_ndcg)/len(_ndcg))\n",
    "    \n",
    "print(sum(fin_hr)/len(fin_hr))\n",
    "print(sum(fin_ndcg)/len(fin_ndcg))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
